{
  "ontology": {
    "AI_HISTORY_FOUNDING": {
      "scope": "Foundational historical facts about AI as a field (e.g., Dartmouth 1956).",
      "typical_confusion": "Mixing up dates/places (e.g., Stanford 1965) or treating AI as starting with deep learning."
    },
    "SYMBOLIC_AI_RULE_BASED": {
      "scope": "The traditional/symbolic approach to AI (explicit if–then rules from expert knowledge).",
      "typical_confusion": "Equating traditional AI with modern data-driven deep learning."
    },
    "TRANSFORMERS_FOUNDATION_MODELS": {
      "scope": "Transformers as the key architecture behind modern generative AI / foundation models (LLMs).",
      "typical_confusion": "Attributing generative AI primarily to CNNs or classical ML like trees/KNN."
    },

    "DATA_TYPE_STRUCTURED_VS_UNSTRUCTURED": {
      "scope": "Distinguishing structured (tabular, fixed schema) vs unstructured (audio, image, text) data and what deep learning can process directly.",
      "typical_confusion": "Thinking SQL tables or spreadsheets are unstructured because they are large."
    },
    "STANDARDIZATION_WHAT_AND_WHY": {
      "scope": "Mean–std scaling (standardization) and why it helps optimization converge faster and more stably by keeping feature scales comparable.",
      "typical_confusion": "Believing standardization guarantees accuracy or prevents overfitting."
    },
    "DATA_LEAKAGE_TRAIN_VS_TEST": {
      "scope": "Fitting preprocessing steps on training data only and applying them unchanged to validation/test data to avoid information leakage.",
      "typical_confusion": "Expecting standardized test data to have exactly mean 0 and std 1."
    },
    "DATA_SPLITS_TRAIN_VAL_TEST": {
      "scope": "Roles of training, validation, and test sets and how parameters like validation_split and test_split are used.",
      "typical_confusion": "Using the test set for tuning or confusing validation loss with test performance."
    },
    "ARCHITECTURE_COMPONENTS_LAYER_ROLES": {
      "scope": "Conceptual roles of input, hidden, and output layers and high-level model structure.",
      "typical_confusion": "Thinking adding more layers or units always improves performance."
    },
    "ACTIVATION_FUNCTION_CHOICES": {
      "scope": "Choosing appropriate activation functions for hidden and output layers and understanding their output ranges.",
      "typical_confusion": "Using ReLU in a probability output layer or sigmoid everywhere by default."
    },
    "SKIP_CONNECTIONS_RESIDUALS": {
      "scope": "Skip connections that feed inputs or earlier representations directly into later layers to preserve information.",
      "typical_confusion": "Thinking skip connections just add more neurons rather than changing information flow."
    },
    "LOSS_FUNCTION_MATCHING_TASK": {
      "scope": "Matching loss functions to the task (e.g., binary cross-entropy for binary classification).",
      "typical_confusion": "Using MSE for classification because labels are numeric."
    },
    "GRADIENT_DESCENT_INTUITION": {
      "scope": "Understanding gradients and learning rate as direction and step size for minimizing loss.",
      "typical_confusion": "Believing gradients point directly to the minimum."
    },
    "EPOCH_BATCH_TERMS": {
      "scope": "Definitions of epoch, batch, and passes through the dataset during training.",
      "typical_confusion": "Thinking an epoch corresponds to a single batch."
    },
    "PARAMETER_COUNTING_DENSE": {
      "scope": "Counting trainable parameters in dense layers using (inputs × units) + bias.",
      "typical_confusion": "Forgetting bias terms or counting neurons instead of parameters."
    },
    "DIMENSION_FLOW_CONCAT_SKIP": {
      "scope": "Tracking dimensionality when inputs are concatenated (e.g., skip connections).",
      "typical_confusion": "Forgetting to add dimensions from multiple sources."
    },
    "CURVES_GENERALIZATION_OVERFITTING": {
      "scope": "Interpreting training vs validation loss curves to assess generalization and overfitting.",
      "typical_confusion": "Assuming lower training loss alone implies better generalization."
    },
    "METRICS_MSE_INTERPRETATION": {
      "scope": "Comparing MSE values across models and interpreting percentage improvements.",
      "typical_confusion": "Misreading magnitudes or signs when comparing metrics."
    }
  },

  "lectures": [
    {
      "lecture_id": "L1_1",
      "lecture_title": "What are Neural Networks?",
      "questions": [
        {
          "question_id": "L1_1_Q1",
          "question_type": "single_select",
          "prompt": "When and where was the field of Artificial Intelligence founded?",
          "options": {
            "A": "1965 at Stanford University",
            "B": "1956 at Dartmouth College",
            "C": "1980 at MIT Sloan",
            "D": "2000 at Harvard University"
          },
          "correct_options": ["B"],
          "concept_tags": ["AI_HISTORY_FOUNDING"],
          "explanation": "The field of Artificial Intelligence was formally founded in 1956 at Dartmouth College, when computer science pioneers met to define the field and give it its name."
        },
        {
          "question_id": "L1_1_Q2",
          "question_type": "single_select",
          "prompt": "What is the traditional approach to AI described in the lecture?",
          "options": {
            "A": "Gather expert knowledge and program it into computers as if-then rules",
            "B": "Collect large datasets and train statistical models",
            "C": "Use neural networks with multiple hidden layers",
            "D": "Generate new data automatically with AI models"
          },
          "correct_options": ["A"],
          "concept_tags": ["SYMBOLIC_AI_RULE_BASED"],
          "explanation": "The lecture describes the traditional approach to AI as interviewing experts, writing down their knowledge in the form of if–then rules, and programming those rules directly into a computer. This rule-based system was a common-sense method in the early years of AI."
        }
      ]
    },
    {
      "lecture_id": "L1_2",
      "lecture_title": "Structured Input Data",
      "questions": [
        {
          "question_id": "L1_2_Q1",
          "question_type": "single_select",
          "prompt": "Which of the following is an example of unstructured data that deep learning can process directly?",
          "options": {
            "A": "Tabular sales records in a spreadsheet",
            "B": "An audio recording of a customer support call",
            "C": "An SQL database of customer IDs",
            "D": "A set of numeric temperature readings"
          },
          "correct_options": ["B"],
          "concept_tags": ["DATA_TYPE_STRUCTURED_VS_UNSTRUCTURED"],
          "explanation": "Deep learning is particularly well-suited for unstructured data types like audio, text, and images, which don’t fit neatly into tabular formats."
        }
      ]
    },
    {
      "lecture_id": "L1_3",
      "lecture_title": "Applications of Neural Networks",
      "questions": [
        {
          "question_id": "L1_3_Q1",
          "question_type": "single_select",
          "prompt": "What AI architecture enabled the rise of generative AI systems like ChatGPT, Claude, and Google Gemini?",
          "options": {
            "A": "CNNs",
            "B": "Transformer models",
            "C": "Decision Trees",
            "D": "KNNs"
          },
          "correct_options": ["B"],
          "concept_tags": ["TRANSFORMERS_FOUNDATION_MODELS"],
          "explanation": "The Transformer architecture is the core model behind modern generative AI, enabling creation of unstructured outputs like text, images, and more."
        }
      ]
    },
    {
      "lecture_id": "L2_1",
      "lecture_title": "What is Deep Learning?",
      "questions": [
        {
          "question_id": "L2_1_Q1",
          "question_type": "single_select",
          "prompt": "In the lecture, what machine learning method do we use as the starting point to introduce neural networks?",
          "options": {
            "A": "Linear regression",
            "B": "Logistic regression",
            "C": "Decision trees",
            "D": "Support vector machines"
          },
          "correct_options": ["B"],
          "concept_tags": ["ARCHITECTURE_COMPONENTS_LAYER_ROLES"],
          "explanation": "The lecture starts from logistic regression and reframes it as a simple network of nodes and connections, motivating how neural networks generalize this idea."
        }
      ]
    },
    {
      "lecture_id": "L2_2",
      "lecture_title": "Viewing Through a 'Lens'",
      "questions": [
        {
          "question_id": "L2_2_Q1",
          "question_type": "single_select",
          "prompt": "Which of the following best describes deep learning in the context of AI?",
          "options": {
            "A": "A rule-based system that relies solely on explicit programming",
            "B": "A subset of machine learning that uses multi-layered neural networks to learn from data",
            "C": "A database query language used for managing big data",
            "D": "A set of prewritten functions for spreadsheet software"
          },
          "correct_options": ["B"],
          "concept_tags": ["ARCHITECTURE_COMPONENTS_LAYER_ROLES"],
          "explanation": "Deep learning is a subset of machine learning based on multi-layer neural networks that learn representations from data."
        },
        {
          "question_id": "L2_2_Q2",
          "question_type": "single_select",
          "prompt": "What is one key advantage of deep learning over traditional machine learning approaches?",
          "options": {
            "A": "It requires no data to train",
            "B": "It is always more accurate regardless of the task",
            "C": "It can automatically learn features from raw data without manual feature engineering",
            "D": "It is designed to only work on tabular data"
          },
          "correct_options": ["C"],
          "concept_tags": ["ARCHITECTURE_COMPONENTS_LAYER_ROLES"],
          "explanation": "Deep learning can learn hierarchical features directly from raw inputs, reducing the need for manual feature engineering."
        }
      ]
    },
    {
      "lecture_id": "L2_3",
      "lecture_title": "Designing a Neural Network",
      "questions": [
        {
          "question_id": "L2_3_Q1",
          "question_type": "single_select",
          "prompt": "According to the lecture, what activation function is recommended as a good default choice for hidden layers in neural networks?",
          "options": {
            "A": "Sigmoid",
            "B": "ReLU",
            "C": "Tanh",
            "D": "Softmax"
          },
          "correct_options": ["B"],
          "concept_tags": ["ACTIVATION_FUNCTION_CHOICES"],
          "explanation": "ReLU is commonly recommended as a default hidden-layer activation because it performs well in practice and is widely used."
        }
      ]
    },
    {
      "lecture_id": "L3_1",
      "lecture_title": "Designing a Neural Network",
      "questions": [
        {
          "question_id": "L3_1_Q1",
          "question_type": "single_select",
          "prompt": "How many input features are used in the heart disease dataset after one-hot encoding?",
          "options": {
            "A": "13",
            "B": "29",
            "C": "16",
            "D": "100"
          },
          "correct_options": ["B"],
          "concept_tags": ["ARCHITECTURE_COMPONENTS_LAYER_ROLES"],
          "explanation": "One-hot encoding expands categorical variables into multiple binary columns, increasing the total feature count to 29."
        }
      ]
    },
    {
      "lecture_id": "L3_2",
      "lecture_title": "Picture to Code",
      "questions": [
        {
          "question_id": "L3_2_Q1",
          "question_type": "single_select",
          "prompt": "Which activation function is used in the output layer for predicting heart disease?",
          "options": {
            "A": "ReLU",
            "B": "Sigmoid",
            "C": "Tanh",
            "D": "Linear"
          },
          "correct_options": ["B"],
          "concept_tags": ["ACTIVATION_FUNCTION_CHOICES", "LOSS_FUNCTION_MATCHING_TASK"],
          "explanation": "For binary classification, the output is a probability in (0,1), so sigmoid is standard; this pairs naturally with binary cross-entropy."
        }
      ]
    },
    {
      "lecture_id": "L3_3",
      "lecture_title": "Recap: What Does Training Mean?",
      "questions": [
        {
          "question_id": "L3_3_Q1",
          "question_type": "single_select",
          "prompt": "Which loss function is used for binary classification problems?",
          "options": {
            "A": "Mean squared error",
            "B": "Binary cross-entropy",
            "C": "Hinge loss",
            "D": "Categorical cross-entropy"
          },
          "correct_options": ["B"],
          "concept_tags": ["LOSS_FUNCTION_MATCHING_TASK"],
          "explanation": "Binary cross-entropy is the standard loss for probabilistic binary classification outputs."
        }
      ]
    },
    {
      "lecture_id": "L3_4",
      "lecture_title": "Minimizing Loss Functions",
      "questions": [
        {
          "question_id": "L3_4_Q1",
          "question_type": "single_select",
          "prompt": "What does the derivative (slope) tell us in gradient descent?",
          "options": {
            "A": "The exact location of the minimum",
            "B": "The direction to adjust parameters",
            "C": "The number of neurons in the layer",
            "D": "The correct batch size to use"
          },
          "correct_options": ["B"],
          "concept_tags": ["GRADIENT_DESCENT_INTUITION"],
          "explanation": "The gradient provides local directional information (increase/decrease) for updating parameters to reduce loss."
        }
      ]
    },
    {
      "lecture_id": "L4_1",
      "lecture_title": "Recap: Gradient Descent",
      "questions": [
        {
          "question_id": "L4_1_Q1",
          "question_type": "single_select",
          "prompt": "What is the learning rate (alpha) in gradient descent used for?",
          "options": {
            "A": "It decides how many hidden layers the network has",
            "B": "It controls how big or small the weight updates are",
            "C": "It sets the number of training epochs",
            "D": "It chooses which loss function to use"
          },
          "correct_options": ["B"],
          "concept_tags": ["GRADIENT_DESCENT_INTUITION"],
          "explanation": "The learning rate controls the step size of parameter updates during optimization."
        }
      ]
    },
    {
      "lecture_id": "L4_2",
      "lecture_title": "Loss Functions",
      "questions": [
        {
          "question_id": "L4_2_Q1",
          "question_type": "single_select",
          "prompt": "What key technique is used to efficiently compute gradients in deep networks?",
          "options": {
            "A": "Forward propagation",
            "B": "Backpropagation",
            "C": "Random search",
            "D": "Clustering"
          },
          "correct_options": ["B"],
          "concept_tags": ["GRADIENT_DESCENT_INTUITION"],
          "explanation": "Backpropagation efficiently computes gradients using the chain rule through the network."
        }
      ]
    },
    {
      "lecture_id": "L4_3",
      "lecture_title": "Neural Network Training",
      "questions": [
        {
          "question_id": "L4_3_Q1",
          "question_type": "single_select",
          "prompt": "What is one pass through the entire training dataset called?",
          "options": {
            "A": "A batch",
            "B": "An epoch",
            "C": "A layer",
            "D": "A fold"
          },
          "correct_options": ["B"],
          "concept_tags": ["EPOCH_BATCH_TERMS"],
          "explanation": "An epoch is one full pass through the training dataset (typically via many mini-batches)."
        }
      ]
    }
  ],

  "assignments": [
    {
      "assignment_id": "A1",
      "assignment_title": "Assignment 1",
      "parts": [
        {
          "part_id": "A1_P1",
          "part_title": "Part 1",
          "questions": [
            {
              "question_id": "A1_P1_Q1",
              "question_type": "single_select",
              "prompt": "Which of the following is the main reason for standardizing input data before training a neural network?",
              "options": {
                "A": "It ensures all features have exactly zero mean and unit variance so that the model always reaches 100% accuracy",
                "B": "It helps the optimizer converge faster and more stably by keeping feature scales similar, preventing some weights from updating much faster than others",
                "C": "It prevents overfitting by adding random noise to the features",
                "D": "It guarantees that the model will not require any regularization"
              },
              "correct_options": ["B"],
              "concept_tags": ["STANDARDIZATION_WHAT_AND_WHY"],
              "explanation": "Standardization helps optimization by putting features on comparable scales; it does not guarantee accuracy or remove the need for regularization."
            },
            {
              "question_id": "A1_P1_Q2",
              "question_type": "single_select",
              "prompt": "How big is our training set and test set? You can use the code output in the notebook to determine the training set size and compute the test set size from the test_split ratio.",
              "options": {
                "A": "Training size: 16,512; test size: 4,128",
                "B": "Training size: 16,512; test size: 20,640",
                "C": "Training size: 20,640; test size: 4,128",
                "D": "Training size: 20,640; test size: 16,512"
              },
              "correct_options": ["A"],
              "concept_tags": ["DATA_SPLITS_TRAIN_VAL_TEST"],
              "explanation": "With test_split=0.2, training is 80% and test is 20% of the full dataset, so test size is training_size/4."
            },
            {
              "question_id": "A1_P1_Q3",
              "question_type": "single_select",
              "prompt": "Why is the mean and standard deviation of the standardized test set close to 0 and 1, respectively, but not exactly 0 and 1?",
              "options": {
                "A": "Scaler fit on training set; test transformed using training stats; test distribution differs slightly so mean/std aren’t exactly 0/1",
                "B": "There was an error in the standardization code",
                "C": "Standardization can never produce values close to 0 and 1",
                "D": "The training set std is forced to be exactly 0"
              },
              "correct_options": ["A"],
              "concept_tags": ["DATA_LEAKAGE_TRAIN_VS_TEST"],
              "explanation": "We fit scaling on training only to prevent leakage; test is transformed using training statistics, so it’s only approximately standardized."
            }
          ]
        },
        {
          "part_id": "A1_P2",
          "part_title": "Part 2",
          "questions": [
            {
              "question_id": "A1_P2_Q1",
              "question_type": "single_select",
              "prompt": "What is the purpose of the 'validation_split' parameter in the model’s '.fit()' function?",
              "options": {
                "A": "To shuffle the dataset before training",
                "B": "To split a portion of the training data for validation during training",
                "C": "To increase the learning rate automatically",
                "D": "To normalize the input data"
              },
              "correct_options": ["B"],
              "concept_tags": ["DATA_SPLITS_TRAIN_VAL_TEST"],
              "explanation": "validation_split reserves part of the training data to evaluate generalization during training without touching the test set."
            },
            {
              "question_id": "A1_P2_Q2",
              "question_type": "single_select",
              "prompt": "During training of the final tuned model, what was the final validation loss?",
              "options": {
                "A": "34,941,157,376",
                "B": "34,543,902,720",
                "C": "35,293,638,656",
                "D": "35,265,150,976"
              },
              "correct_options": ["A"],
              "concept_tags": ["DATA_SPLITS_TRAIN_VAL_TEST"],
              "explanation": "This is reading the final validation loss value from the training history (last epoch)."
            }
          ]
        },
        {
          "part_id": "A1_P3",
          "part_title": "Part 3",
          "questions": [
            {
              "question_id": "A1_P3_Q1",
              "question_type": "single_select",
              "prompt": "If we increase the number of units in the second hidden layer of Model 2 from 16 to 32, which change occurs in total trainable parameters?",
              "options": {
                "A": "Stay the same",
                "B": "Increase",
                "C": "Decrease",
                "D": "Become zero"
              },
              "correct_options": ["B"],
              "concept_tags": ["PARAMETER_COUNTING_DENSE"],
              "explanation": "More units means more weights and biases, increasing trainable parameters."
            },
            {
              "question_id": "A1_P3_Q2",
              "question_type": "numeric_input",
              "prompt": "If we did as described in Question 2.1, how many trainable parameters would the second hidden layer have?",
              "correct_answer": 544,
              "tolerance": 0,
              "concept_tags": ["PARAMETER_COUNTING_DENSE"],
              "explanation": "(inputs×units)+bias = (16×32)+32 = 544."
            },
            {
              "question_id": "A1_P3_Q3",
              "question_type": "single_select",
              "prompt": "Based on the loss curves for Model 1 and Model 2, which statement is most accurate?",
              "options": {
                "A": "Model 2 converges faster and reaches a lower validation loss than Model 1",
                "B": "Model 1 converges faster and reaches a lower validation loss than Model 2",
                "C": "Both converge at the same rate and reach identical final validation losses",
                "D": "Model 2 shows severe overfitting vs Model 1"
              },
              "correct_options": ["A"],
              "concept_tags": ["CURVES_GENERALIZATION_OVERFITTING"],
              "explanation": "You interpret validation-loss curves to compare convergence and generalization."
            },
            {
              "question_id": "A1_P3_Q4",
              "question_type": "single_select",
              "prompt": "What is the percentage improvement in Model 2’s performance compared to Model 1 based on test MSE?",
              "options": {
                "A": "86.32%",
                "B": "-86.32%",
                "C": "-0.8632%",
                "D": "0.8632%"
              },
              "correct_options": ["A"],
              "concept_tags": ["METRICS_MSE_INTERPRETATION"],
              "explanation": "Compute percent improvement using ((MSE1−MSE2)/MSE1)×100 and interpret magnitude/sign correctly."
            }
          ]
        },
        {
          "part_id": "A1_P4",
          "part_title": "Part 4",
          "questions": [
            {
              "question_id": "A1_P4_Q1",
              "question_type": "single_select",
              "prompt": "What is the main structural difference in the Skip Connection model compared to the other models?",
              "options": {
                "A": "Uses convolutional layers",
                "B": "Adds original inputs directly to later computations (skip connection)",
                "C": "Increases units in each hidden layer",
                "D": "Removes activation functions"
              },
              "correct_options": ["B"],
              "concept_tags": ["SKIP_CONNECTIONS_RESIDUALS"],
              "explanation": "Skip connections change information flow by letting earlier representations feed into later layers."
            },
            {
              "question_id": "A1_P4_Q2",
              "question_type": "numeric_input",
              "prompt": "How many input connections feed into the output layer of our Skip Connection model?",
              "correct_answer": 24,
              "tolerance": 0,
              "concept_tags": ["DIMENSION_FLOW_CONCAT_SKIP"],
              "explanation": "Output receives 8 (inputs) + 16 (hidden) = 24 incoming units."
            },
            {
              "question_id": "A1_P4_Q3",
              "question_type": "numeric_input",
              "prompt": "How many trainable parameters are associated with the output layer?",
              "correct_answer": 25,
              "tolerance": 0,
              "concept_tags": ["PARAMETER_COUNTING_DENSE", "DIMENSION_FLOW_CONCAT_SKIP"],
              "explanation": "With 24 incoming units and 1 output unit: 24×1 + 1 = 25."
            },
            {
              "question_id": "A1_P4_Q4",
              "question_type": "numeric_input",
              "prompt": "How many trainable parameters does our Skip Connection model have in total?",
              "correct_answer": 169,
              "tolerance": 0,
              "concept_tags": ["PARAMETER_COUNTING_DENSE", "DIMENSION_FLOW_CONCAT_SKIP"],
              "explanation": "Hidden: 8×16+16=144; Output: 25; Total 169."
            },
            {
              "question_id": "A1_P4_Q5",
              "question_type": "single_select",
              "prompt": "What pattern in the loss curves best describes the Skip Connection model compared to the others?",
              "options": {
                "A": "Overfits significantly more",
                "B": "Reaches a lower validation loss",
                "C": "Shows higher variance between epochs"
              },
              "correct_options": ["B"],
              "concept_tags": ["CURVES_GENERALIZATION_OVERFITTING"],
              "explanation": "This is interpreting validation-loss curves to compare generalization."
            },
            {
              "question_id": "A1_P4_Q6",
              "question_type": "single_select",
              "prompt": "Which model has achieved the lowest test MSE?",
              "options": {
                "A": "Model 1",
                "B": "Model 2",
                "C": "Model 3",
                "D": "None"
              },
              "correct_options": ["B"],
              "concept_tags": ["METRICS_MSE_INTERPRETATION"],
              "explanation": "Pick the smallest test MSE and interpret it as best test performance."
            },
            {
              "question_id": "A1_P4_Q7",
              "question_type": "single_select",
              "prompt": "Which model appears to have the most clear signs of overfitting?",
              "options": {
                "A": "Model 1",
                "B": "Model 2",
                "C": "Model 3",
                "D": "None",
                "E": "All"
              },
              "correct_options": ["D"],
              "concept_tags": ["CURVES_GENERALIZATION_OVERFITTING"],
              "explanation": "Overfitting is diagnosed via divergence of training vs validation curves, not just low training loss."
            }
          ]
        }
      ]
    }
  ]
}
